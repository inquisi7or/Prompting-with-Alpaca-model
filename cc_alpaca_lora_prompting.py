# -*- coding: utf-8 -*-
"""CC-Alpaca-LoRA_prompting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18bG7kGQwlIMJLM7pR7VtZr8PpfWoGW9e

Notebook adapted from https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_06_3_alpaca_lora.ipynb#scrollTo=HtcAryvO8C7D
"""

!pip install bitsandbytes
!pip install -q datasets loralib sentencepiece
!pip install -q git+https://github.com/zphang/transformers@c3dc391
!pip install -q git+https://github.com/huggingface/peft.git

"""The following code loads the Alpaca LoRA pretrained model."""

from peft import PeftModel
from transformers import LLaMATokenizer, LLaMAForCausalLM, GenerationConfig

tokenizer = LLaMATokenizer.from_pretrained("chainyo/alpaca-lora-7b")
model = LLaMAForCausalLM.from_pretrained(
    "chainyo/alpaca-lora-7b",
    load_in_8bit=True,
    device_map="auto",
)
#model = PeftModel.from_pretrained(model, "chainyo/alpaca-lora-7b")

"""We provide a utility to build prompts. We allow either a singular prompt, or a prompt with instructions."""

def generate_prompt(instruction, input=None):
    if input:
        return f"""Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Input:
{input}

### Response:"""
    else:
        return f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:"""

"""The following code provides an evaluate function that we will use to call Alpaca LoRA."""

generation_config = GenerationConfig(
    temperature=0.5,
    top_p=0.75,
    num_beams=4,
)

def evaluate(instruction, input=None):
    out_string =""
    prompt = generate_prompt(instruction, input)
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs["input_ids"].cuda()
    generation_output = model.generate(
        input_ids=input_ids,
        generation_config=generation_config,
        return_dict_in_generate=True,
        output_scores=True,
        max_new_tokens=256
    )
    for s in generation_output.sequences:
        output = tokenizer.decode(s)
        out_string += output.split("### Response:")[1].strip()
    return out_string

"""With the following code, you can get a response from a prompt.
Just modify the 'prompt' based on your needs.
You will get the answer in 'response'.

"""

#how to create 5 different stories as an output based on user input in var1, var2, var3, var4 and var5 (they define literary variations) and all of them are based on title and genre user input from the inp variable

inp = input("enter desired title and genre: ")
var1 = input("enter literary variation 1: ")
var2 = input("enter literary variation 2: ")
var3 = input("enter literary variation 3: ")
var4 = input("enter literary variation 4: ")
var5 = input("enter literary variation 5: ")

variations = [var1, var2, var3, var4, var5]

for i, variation in enumerate(variations, 1):
    prompt = f"Write a short 60-word story based on {inp} - Variation {i}: {variation}"
    response = evaluate(prompt)
    print(f"Story {i}:\n{response}\n")

#prompt = "write a short 60 words story based on " + inp
#response = evaluate(prompt)
#print(response)